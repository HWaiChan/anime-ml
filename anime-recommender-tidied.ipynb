{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse.linalg import svds\n",
    "import surprise as sp\n",
    "import time\n",
    "print(\"Setup Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UsersDF = pd.read_csv('./data/full/users_cleaned.csv')\n",
    "AnimesDF = pd.read_csv('./data/full/anime_cleaned.csv')\n",
    "ScoresDF = pd.read_csv('./data/full/animelists_cleaned.csv')\n",
    "\n",
    "ScoresDF_lite = ScoresDF[['username', 'anime_id', 'my_score', 'my_status']]\n",
    "\n",
    "AnimeDF_name_only = AnimesDF[['anime_id', 'title']]\n",
    "ScoresDF_lite_with_names = ScoresDF_lite.merge(AnimeDF_name_only, left_on='anime_id', right_on='anime_id')\n",
    "\n",
    "ScoresDF_lite_with_names_no_0_score = ScoresDF_lite_with_names[ScoresDF_lite_with_names['my_score'] > 0]\n",
    "\n",
    "\n",
    "UsersRatedPerAnime = ScoresDF_lite_with_names_no_0_score['anime_id'].value_counts().reset_index().rename(columns={\"anime_id\": \"number_of_users\", \"index\": \"anime_id\"})\n",
    "AnimesRatedPerUser = ScoresDF_lite_with_names_no_0_score['username'].value_counts().reset_index().rename(columns={\"username\": \"number_of_animes\", \"index\": \"username\"})\n",
    "\n",
    "\n",
    "UserRatedsPerAnimeNice = UsersRatedPerAnime[UsersRatedPerAnime['number_of_users'] > 10]\n",
    "AnimesRatedPerUserNice = AnimesRatedPerUser[AnimesRatedPerUser['number_of_animes'] > 10]\n",
    "\n",
    "ScoresDFFilteredNice = pd.merge(ScoresDF_lite_with_names_no_0_score, AnimesRatedPerUserNice, left_on = 'username', right_on = 'username', how = 'inner')\n",
    "ScoresDFFilteredNice = pd.merge(ScoresDFFilteredNice, UserRatedsPerAnimeNice, left_on = 'anime_id', right_on = 'anime_id', how = 'inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose the correct model to train on\n",
    "- Output the nearest neighbours similar to how I did it for the others\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sp.Reader(rating_scale=(1, 10))\n",
    "data = sp.Dataset.load_from_df(ScoresDFFilteredNice[['username', 'title', 'my_score']], reader)\n",
    "\n",
    "trainset, testset = sp.model_selection.split.train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold= 7):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    estimate_actual_by_user = defaultdict(list)\n",
    "    for user, _, true_rating, estimated_rating, _ in predictions:\n",
    "        estimate_actual_by_user[user].append((estimated_rating, true_rating))\n",
    "    # Creates a dict with the key being a user and the value bringing the estimated rating and the true rating.\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for user, user_ratings in estimate_actual_by_user.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_rating >= threshold) for (_, true_rating) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_rating >= threshold) and (est >= threshold))\n",
    "                              for (est, true_rating) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[user] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[user] = n_rel_and_rec_k /  n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting:  KNNBasic\n",
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "from surprise import dump\n",
    "import os\n",
    "\n",
    "analysis = defaultdict(list)\n",
    "model_filename_prefix = \"./model_\"\n",
    "model_filename_suffix = \".pickle\"\n",
    "def get_model_filename(model_name):\n",
    "    return model_filename_prefix + model_name + model_filename_suffix\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False,  # compute  similarities between items\n",
    "               }\n",
    "'''sp.SVD(), sp.SlopeOne(), sp.NMF(), sp.NormalPredictor(), sp.KNNBaseline(), '''\n",
    "#algorithms = [sp.SVD(), sp.SVDpp(), sp.SlopeOne(), sp.NMF(), sp.NormalPredictor(), sp.KNNBaseline(), sp.KNNBasic(), sp.KNNWithMeans(), sp.KNNWithZScore(), sp.BaselineOnly(), sp.CoClustering()]\n",
    "algorithms = [sp.KNNBasic(sim_options=sim_options), sp.KNNWithMeans(sim_options=sim_options), sp.KNNWithZScore(sim_options=sim_options), sp.CoClustering()]\n",
    "\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    name = algorithm.__class__.__name__\n",
    "    print(\"Starting: \" , name)\n",
    "    start = time.time()    \n",
    "    algorithm.fit(trainset)\n",
    "    predictions = algorithm.test(testset)\n",
    "\n",
    "    rmse = sp.accuracy.rmse(predictions)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=7)\n",
    "    precision_avg = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "\n",
    "    analysis[name] = (name, rmse, precision_avg, time.time() - start)\n",
    "    # Dump algorithm and reload it.\n",
    "    file_name = os.path.expanduser(get_model_filename(name))\n",
    "    dump.dump(file_name, algo=algorithm)\n",
    "    print(\"Done: \" , name, \"\\n\")\n",
    "print ('\\n\\tDONE\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading dumps\n",
      ">> Loaded dump\n",
      "SVD\n"
     ]
    }
   ],
   "source": [
    "print (\">> Loading dumps\")\n",
    "from surprise import dump\n",
    "import os\n",
    "model_filename = \"./model.pickle\"\n",
    "\n",
    "directory = os.fsencode('.')\n",
    "#loaded_models = defaultdict(list)\n",
    "    \n",
    "##for file in os.listdir(directory):\n",
    "##     filename = os.fsdecode(file)\n",
    "##     if filename.endswith(\".pickle\") and filename.startswith(\"model_\"): \n",
    "##         print(filename)\n",
    "##         continue\n",
    "##     else:\n",
    "##         continue\n",
    "\n",
    "file_name = os.path.expanduser(model_filename)\n",
    "_, loaded_model = dump.load(file_name)\n",
    "print (\">> Loaded dump\")\n",
    "print(loaded_model.__class__.__name__)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#analysis_df = pd.DataFrame.from_dict(analysis, orient = 'index', columns = ['Algorithm', 'RMSE', 'Precision@10', 'Time to run (in seconds)']).reset_index()\n",
    "#\n",
    "#analysis_df = analysis_df[['Algorithm', 'RMSE', 'Precision@10', 'Time to run (in seconds)']]\n",
    "#analysis_df = analysis_df.sort_values(by=['Precision@10'], ascending = False)\n",
    "#analysis_df['RMSE^-1'] = analysis_df['RMSE'] ** -1\n",
    "#analysis_df.head(n = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UserRatedsPerAnimeNice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hansw\\Projects\\anime-recommender\\anime-recommender-tidied.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hansw/Projects/anime-recommender/anime-recommender-tidied.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m#loaded_model.predict('Tomoki-sama','Bleach').est\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hansw/Projects/anime-recommender/anime-recommender-tidied.ipynb#ch0000007?line=1'>2</a>\u001b[0m UserRatedsPerAnimeNice\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UserRatedsPerAnimeNice' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#loaded_model.predict('Tomoki-sama','Bleach').est\n",
    "UserRatedsPerAnimeNice.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfcfc0e0e8339771dce3d0e8cf371d6c4aa071b0e726c111f60290ab4202ecce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
